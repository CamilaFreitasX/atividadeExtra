import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from csv_agent import CSVAgent
from data_analyzer import DataAnalyzer
from visualization import DataVisualizer
import os
from io import StringIO
import traceback
import matplotlib
import tempfile
import math

# Configura√ß√µes para produ√ß√£o (Render)
matplotlib.use('Agg')  # Backend n√£o-interativo para matplotlib
os.environ['MPLBACKEND'] = 'Agg'

# Configurar diret√≥rio tempor√°rio para matplotlib
temp_dir = tempfile.mkdtemp()
os.environ['MPLCONFIGDIR'] = temp_dir

# Configura√ß√£o da p√°gina
st.set_page_config(
    page_title="Agente de An√°lise CSV",
    page_icon="üìä",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS customizado
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .section-header {
        font-size: 1.5rem;
        color: #2e8b57;
        margin-top: 2rem;
        margin-bottom: 1rem;
    }
    .metric-card {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 0.5rem 0;
    }
    .insight-box {
        background-color: #e8f4fd;
        padding: 1rem;
        border-left: 4px solid #1f77b4;
        margin: 1rem 0;
    }
    .conclusion-box {
        background-color: #f0f8e8;
        padding: 1rem;
        border-left: 4px solid #2e8b57;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)

def initialize_session_state():
    """Inicializa vari√°veis de sess√£o"""
    if 'agent' not in st.session_state:
        st.session_state.agent = None
    if 'df' not in st.session_state:
        st.session_state.df = None
    if 'analysis_history' not in st.session_state:
        st.session_state.analysis_history = []
    if 'insights_generated' not in st.session_state:
        st.session_state.insights_generated = False
    if 'visualization_cache' not in st.session_state:
        st.session_state.visualization_cache = {}

def load_csv_file(uploaded_file):
    """Carrega arquivo CSV com particionamento autom√°tico para arquivos grandes"""
    try:
        st.info(f"üîç Processando arquivo: {uploaded_file.name}")
        
        # Verificar tamanho do arquivo
        file_size_mb = uploaded_file.size / (1024 * 1024)
        st.info(f"üìä Tamanho do arquivo: {file_size_mb:.2f}MB")
        
        # Verifica√ß√µes de seguran√ßa - Limite aumentado para Render
        if file_size_mb > 200:
            st.error(f"‚ùå Arquivo muito grande ({file_size_mb:.1f}MB). Limite m√°ximo: 200MB")
            st.info("üí° Para arquivos extremamente grandes, considere dividir em partes menores.")
            return None
        
        # Verificar formato
        if not uploaded_file.name.endswith('.csv'):
            st.error("‚ùå Por favor, fa√ßa upload de um arquivo CSV.")
            return None
        
        # Sistema de particionamento autom√°tico para arquivos muito grandes
        if file_size_mb > 50:
            st.warning(f"üîÑ Arquivo muito grande detectado ({file_size_mb:.1f}MB). Aplicando particionamento autom√°tico...")
            
            # Usar sistema de particionamento
            partitions = partition_large_file(uploaded_file, max_partition_size_mb=10)
            
            if not partitions:
                st.error("‚ùå Erro ao particionar arquivo.")
                return None
            
            # Processar parti√ß√µes sequencialmente
            results = process_partitioned_data(partitions, operation="analyze")
            
            if results and results['combined_sample'] is not None:
                df = results['combined_sample']
                st.success(f"‚úÖ Arquivo particionado e processado! Total: {results['total_rows']:,} linhas")
                st.info(f"üìã Amostra combinada: {len(df):,} linhas de {results['total_columns']} colunas")
            else:
                st.error("‚ùå Erro ao processar parti√ß√µes.")
                return None
                
        # Processamento otimizado por chunks para arquivos grandes (20-50MB)
        elif file_size_mb > 20:
            st.warning(f"‚ö†Ô∏è Arquivo grande detectado ({file_size_mb:.1f}MB). Aplicando processamento em chunks...")
            
            # Ler amostra primeiro para detectar estrutura
            try:
                sample_df = pd.read_csv(uploaded_file, nrows=1000, low_memory=False)
                st.info(f"üìã Estrutura detectada: {sample_df.shape[1]} colunas")
                
                # Resetar ponteiro do arquivo
                uploaded_file.seek(0)
                
                # Processamento em chunks para arquivos muito grandes
                chunk_size = 2000  # Chunks menores para arquivos grandes
                chunks = []
                total_rows = 0
                
                st.info(f"üîÑ Processando arquivo em chunks de {chunk_size:,} linhas...")
                
                # Ler arquivo em chunks
                progress_container = st.container()
                with progress_container:
                    chunk_progress = st.progress(0)
                    chunk_status = st.empty()
                
                for i, chunk in enumerate(pd.read_csv(uploaded_file, chunksize=chunk_size, low_memory=False)):
                    chunks.append(chunk)
                    total_rows += len(chunk)
                    
                    # Atualizar progress bar
                    progress_percent = min(total_rows / 30000, 1.0)  # M√°ximo 30k linhas
                    chunk_progress.progress(progress_percent)
                    chunk_status.text(f"üìä Processando chunk {i+1}: {total_rows:,} linhas carregadas")
                    
                    # Limitar n√∫mero total de linhas para evitar problemas de mem√≥ria
                    if total_rows > 30000:  # Limite m√°ximo de 30k linhas
                        chunk_status.text(f"‚ö†Ô∏è Limitando processamento a {total_rows:,} linhas para otimizar performance.")
                        break
                
                # Combinar chunks
                st.info("üîó Combinando dados processados...")
                df = pd.concat(chunks, ignore_index=True)
                
            except Exception as e:
                st.error(f"‚ùå Erro ao processar arquivo em chunks: {str(e)}")
                return None
                
        # Processamento inteligente baseado no tamanho para arquivos m√©dios
        elif file_size_mb > 5:
            st.warning(f"‚ö†Ô∏è Arquivo m√©dio-grande detectado ({file_size_mb:.1f}MB). Aplicando processamento em chunks...")
            
            # Ler amostra primeiro para detectar estrutura
            try:
                sample_df = pd.read_csv(uploaded_file, nrows=1000, low_memory=False)
                st.info(f"üìã Estrutura detectada: {sample_df.shape[1]} colunas")
                
                # Resetar ponteiro do arquivo
                uploaded_file.seek(0)
                
                # Processamento em chunks para arquivos muito grandes
                chunk_size = 5000 if file_size_mb > 20 else 10000
                chunks = []
                total_rows = 0
                
                st.info(f"üîÑ Processando arquivo em chunks de {chunk_size:,} linhas...")
                
                # Ler arquivo em chunks
                progress_container = st.container()
                with progress_container:
                    chunk_progress = st.progress(0)
                    chunk_status = st.empty()
                
                for i, chunk in enumerate(pd.read_csv(uploaded_file, chunksize=chunk_size, low_memory=False)):
                    chunks.append(chunk)
                    total_rows += len(chunk)
                    
                    # Atualizar progress bar
                    progress_percent = min(total_rows / 50000, 1.0)  # M√°ximo 50k linhas
                    chunk_progress.progress(progress_percent)
                    chunk_status.text(f"üìä Processando chunk {i+1}: {total_rows:,} linhas carregadas")
                    
                    # Limitar n√∫mero total de linhas para evitar problemas de mem√≥ria
                    if total_rows > 50000:  # Limite m√°ximo de 50k linhas
                        chunk_status.text(f"‚ö†Ô∏è Limitando processamento a {total_rows:,} linhas para otimizar performance.")
                        break
                
                # Combinar chunks
                st.info("üîó Combinando dados processados...")
                df = pd.concat(chunks, ignore_index=True)
                
            except Exception as e:
                st.error(f"‚ùå Erro ao processar arquivo em chunks: {str(e)}")
                return None
                
        elif file_size_mb > 2:
            st.info(f"üìä Arquivo m√©dio detectado ({file_size_mb:.1f}MB). Aplicando otimiza√ß√µes...")
            
            # Ler amostra primeiro para detectar estrutura
            try:
                sample_df = pd.read_csv(uploaded_file, nrows=500, low_memory=False)
                st.info(f"üìã Estrutura detectada: {sample_df.shape[1]} colunas")
                
                # Resetar ponteiro do arquivo
                uploaded_file.seek(0)
                
                # Ler com otimiza√ß√µes de mem√≥ria
                df = pd.read_csv(uploaded_file, 
                               low_memory=False,
                               engine='c',  # Engine mais r√°pida
                               memory_map=True)  # Usar memory mapping
                
                # Para arquivos m√©dios, usar amostragem inteligente se necess√°rio
                if len(df) > 20000:
                    st.warning(f"‚ö†Ô∏è Dataset com {len(df):,} linhas. Usando amostra estratificada de 20.000 linhas.")
                    df = df.sample(n=20000, random_state=42).reset_index(drop=True)
                    
            except Exception as e:
                st.error(f"‚ùå Erro ao processar arquivo grande: {str(e)}")
                return None
        else:
            # Arquivos pequenos - processamento normal
            df = pd.read_csv(uploaded_file, low_memory=False)
        
        st.success(f"‚úÖ Arquivo carregado! Shape: {df.shape}")
        st.info(f"üìã Colunas: {list(df.columns[:8])}{'...' if len(df.columns) > 8 else ''}")
        
        # Otimizar tipos de dados para economizar mem√≥ria
        original_memory = df.memory_usage(deep=True).sum() / (1024**2)
        
        for col in df.columns:
            if df[col].dtype == 'object':
                try:
                    # Converter para categoria se tiver poucos valores √∫nicos
                    unique_ratio = df[col].nunique() / len(df)
                    if unique_ratio < 0.5 and df[col].nunique() < 1000:
                        df[col] = df[col].astype('category')
                except:
                    pass
            elif df[col].dtype in ['int64', 'float64']:
                try:
                    # Otimizar tipos num√©ricos
                    if df[col].dtype == 'int64':
                        if df[col].min() >= 0 and df[col].max() <= 255:
                            df[col] = df[col].astype('uint8')
                        elif df[col].min() >= -128 and df[col].max() <= 127:
                            df[col] = df[col].astype('int8')
                        elif df[col].min() >= -32768 and df[col].max() <= 32767:
                            df[col] = df[col].astype('int16')
                        elif df[col].min() >= -2147483648 and df[col].max() <= 2147483647:
                            df[col] = df[col].astype('int32')
                    elif df[col].dtype == 'float64':
                        # Tentar converter para float32 se n√£o houver perda de precis√£o
                        if df[col].equals(df[col].astype('float32').astype('float64')):
                            df[col] = df[col].astype('float32')
                except:
                    pass
        
        optimized_memory = df.memory_usage(deep=True).sum() / (1024**2)
        reduction = ((original_memory - optimized_memory) / original_memory) * 100
        
        st.info(f"üíæ Mem√≥ria otimizada: {optimized_memory:.1f}MB (redu√ß√£o de {reduction:.1f}%)")
        
        return df
        
    except pd.errors.EmptyDataError:
        st.error("‚ùå Arquivo CSV est√° vazio.")
        return None
    except pd.errors.ParserError as e:
        st.error(f"‚ùå Erro ao analisar CSV: {str(e)}")
        st.info("üí° Verifique se o arquivo est√° no formato CSV correto.")
        return None
    except MemoryError:
        st.error("‚ùå Erro de mem√≥ria: Arquivo muito grande para processar.")
        st.info("üí° Tente usar uma amostra menor do dataset.")
        return None
    except Exception as e:
        st.error(f"‚ùå Erro inesperado ao carregar arquivo: {str(e)}")
        st.error(f"üîç Detalhes t√©cnicos: {traceback.format_exc()}")
        return None

def display_dataset_overview(df):
    """Exibe vis√£o geral do dataset"""
    st.markdown('<div class="section-header">üìã Vis√£o Geral do Dataset</div>', unsafe_allow_html=True)
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Linhas", f"{len(df):,}")
    with col2:
        st.metric("Colunas", len(df.columns))
    with col3:
        st.metric("Valores Faltantes", f"{df.isnull().sum().sum():,}")
    with col4:
        st.metric("Tamanho (MB)", f"{df.memory_usage(deep=True).sum() / 1024**2:.2f}")
    
    # Informa√ß√µes das colunas
    st.markdown("**Informa√ß√µes das Colunas:**")
    col_info = pd.DataFrame({
        'Coluna': df.columns,
        'Tipo': df.dtypes.astype(str),
        'Valores √önicos': [df[col].nunique() for col in df.columns],
        'Valores Faltantes': [df[col].isnull().sum() for col in df.columns],
        '% Faltantes': [f"{(df[col].isnull().sum() / len(df)) * 100:.1f}%" for col in df.columns]
    })
    st.dataframe(col_info, use_container_width=True)
    
    # Amostra dos dados
    st.markdown("**Amostra dos Dados:**")
    st.dataframe(df.head(10), use_container_width=True)

def display_automatic_analysis(agent):
    """Exibe an√°lise autom√°tica do dataset"""
    st.markdown('<div class="section-header">üîç An√°lise Autom√°tica</div>', unsafe_allow_html=True)
    
    try:
        overview = agent.get_dataset_overview()
        
        if 'data_quality' in overview:
            quality = overview['data_quality']
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("**Qualidade dos Dados:**")
                st.metric("Completude dos Dados", f"{quality.get('data_completeness', 0):.1f}%")
                st.metric("Linhas Duplicadas", quality.get('duplicate_rows', 0))
            
            with col2:
                st.markdown("**Distribui√ß√£o de Tipos:**")
                if agent.analyzer:
                    st.metric("Colunas Num√©ricas", len(agent.analyzer.numeric_columns))
                    st.metric("Colunas Categ√≥ricas", len(agent.analyzer.categorical_columns))
        
        # Gerar insights autom√°ticos
        if not st.session_state.insights_generated:
            with st.spinner("Gerando insights autom√°ticos..."):
                insights = agent.generate_insights()
                if insights:
                    st.markdown("**Insights Autom√°ticos:**")
                    for i, insight in enumerate(insights, 1):
                        st.markdown(f'<div class="insight-box"><strong>Insight {i}:</strong> {insight}</div>', 
                                  unsafe_allow_html=True)
                st.session_state.insights_generated = True
    
    except Exception as e:
        st.error(f"Erro na an√°lise autom√°tica: {str(e)}")

def display_visualizations(agent):
    """Exibe visualiza√ß√µes autom√°ticas"""
    st.markdown('<div class="section-header">üìä Visualiza√ß√µes Autom√°ticas</div>', unsafe_allow_html=True)
    
    try:
        if agent.visualizer:
            # Vis√£o geral do dashboard
            info_text, figs = agent.visualizer.create_dashboard_overview()
            
            st.markdown(info_text)
            
            # Exibir gr√°ficos
            for i, fig in enumerate(figs):
                if fig:
                    st.plotly_chart(fig, use_container_width=True, key=f"dashboard_chart_{i}")
                    
                    # Limitar n√∫mero de gr√°ficos para n√£o sobrecarregar
                    if i >= 3:
                        break
    
    except Exception as e:
        st.error(f"Erro ao gerar visualiza√ß√µes: {str(e)}")

def handle_user_question(agent, question):
    """Processa pergunta do usu√°rio"""
    try:
        # Verificar cache de visualiza√ß√µes
        question_hash = hash(question.lower().strip())
        if question_hash in st.session_state.visualization_cache:
            response, visualization = st.session_state.visualization_cache[question_hash]
            st.info("üìã Resposta recuperada do cache (mais r√°pido!)")
        else:
            with st.spinner("Analisando sua pergunta..."):
                response, visualization = agent.ask_question(question)
                # Salvar no cache
                st.session_state.visualization_cache[question_hash] = (response, visualization)
        
        # Exibir resposta
        st.markdown("**Resposta:**")
        st.write(response)
        
        # Exibir visualiza√ß√£o se dispon√≠vel
        if visualization:
            st.markdown("**Visualiza√ß√£o:**")
            import time
            st.plotly_chart(visualization, use_container_width=True, key=f"question_chart_{int(time.time() * 1000)}")
        else:
            st.info("‚ÑπÔ∏è Nenhuma visualiza√ß√£o foi gerada para esta pergunta.")
        
        # Adicionar ao hist√≥rico
        st.session_state.analysis_history.append({
            'question': question,
            'response': response,
            'has_visualization': visualization is not None
        })
        
        return True
    
    except Exception as e:
        st.error(f"Erro ao processar pergunta: {str(e)}")
        st.error(f"Detalhes: {traceback.format_exc()}")
        return False

def display_conclusions(agent):
    """Exibe conclus√µes do agente"""
    st.markdown('<div class="section-header">üí° Conclus√µes do Agente</div>', unsafe_allow_html=True)
    
    try:
        conclusions = agent.generate_conclusions()
        
        if conclusions:
            for i, conclusion in enumerate(conclusions, 1):
                st.markdown(f'<div class="conclusion-box"><strong>Conclus√£o {i}:</strong> {conclusion}</div>', 
                          unsafe_allow_html=True)
        else:
            st.info("Fa√ßa algumas perguntas para que o agente possa gerar conclus√µes baseadas nas an√°lises.")
    
    except Exception as e:
        st.error(f"Erro ao gerar conclus√µes: {str(e)}")

def main():
    """Fun√ß√£o principal da aplica√ß√£o"""
    try:
        initialize_session_state()
        
        # Header
        st.markdown('<div class="main-header">ü§ñ Agente Inteligente de An√°lise CSV</div>', unsafe_allow_html=True)
        st.markdown("---")
        
        # Verificar recursos do sistema (opcional)
        try:
            import psutil
            memory_percent = psutil.virtual_memory().percent
            if memory_percent > 85:
                st.warning(f"‚ö†Ô∏è Uso de mem√≥ria alto ({memory_percent:.1f}%). Performance pode ser afetada.")
        except ImportError:
            # psutil n√£o dispon√≠vel, continuar sem monitoramento
            pass
        except Exception:
            # Erro ao acessar informa√ß√µes do sistema, continuar normalmente
            pass
    
    except Exception as e:
        st.error(f"Erro na inicializa√ß√£o da aplica√ß√£o: {str(e)}")
        st.error("Tente recarregar a p√°gina ou reiniciar a sess√£o.")
        return
    
    # Sidebar para configura√ß√µes
    with st.sidebar:
        st.header("‚öôÔ∏è Configura√ß√µes")
        
        # Upload de arquivo
        st.subheader("üìÅ Upload de Arquivo")
        uploaded_file = st.file_uploader(
            "Escolha um arquivo CSV",
            type=['csv'],
            help="Fa√ßa upload de um arquivo CSV para an√°lise"
        )
        
        # Bot√£o para limpar arquivo atual
        if uploaded_file is not None:
            if st.button("üóëÔ∏è Limpar Arquivo", help="Remove o arquivo atual e permite selecionar outro"):
                st.session_state.clear()
                st.rerun()
        
        # Configura√ß√£o da API
        st.subheader("üîë Configura√ß√£o da API")
        api_key = st.text_input(
            "OpenAI API Key",
            type="password",
            help="Insira sua chave da API OpenAI"
        )
        
        if st.button("üîÑ Reiniciar Sess√£o"):
            st.session_state.clear()
            st.rerun()
    
    # Verificar se API key est√° configurada
    effective_api_key = api_key or os.getenv("OPENAI_API_KEY")
    
    # Validar se a API key tem formato v√°lido (come√ßa com sk-)
    def is_valid_key(key):
        return key and key.startswith("sk-") and len(key) > 20
    
    # Configurar agente com API key ou modo demonstra√ß√£o (apenas se n√£o existir)
    if st.session_state.agent is None:
        if effective_api_key and is_valid_key(effective_api_key):
            try:
                st.session_state.agent = CSVAgent(api_key=effective_api_key)
                st.success("‚úÖ API Key configurada com sucesso!")
            except Exception as e:
                st.warning(f"‚ö†Ô∏è Erro ao conectar com OpenAI: {str(e)}")
                st.info("üéØ Ativando modo de demonstra√ß√£o...")
                st.session_state.agent = CSVAgent(demo_mode=True)
        else:
            if effective_api_key:
                st.warning("‚ö†Ô∏è API Key inv√°lida. Ativando modo de demonstra√ß√£o...")
            else:
                st.info("‚ÑπÔ∏è API Key n√£o configurada. Ativando modo de demonstra√ß√£o...")
            st.session_state.agent = CSVAgent(demo_mode=True)
    
    # Processar upload de arquivo (apenas se for um arquivo novo)
    if uploaded_file is not None:
        # Verificar se √© um arquivo novo
        current_file_info = f"{uploaded_file.name}_{uploaded_file.size}"
        if 'current_file_info' not in st.session_state or st.session_state.current_file_info != current_file_info:
            st.info(f"üìÅ Arquivo detectado: {uploaded_file.name}")
            
            # Verificar tamanho antes do processamento
            file_size_mb = uploaded_file.size / (1024 * 1024)
            if file_size_mb > 200:
                st.error(f"‚ùå Arquivo muito grande ({file_size_mb:.1f}MB). Limite m√°ximo: 200MB")
                
                # Mostrar op√ß√µes para o usu√°rio
                st.markdown("### üîß Op√ß√µes para arquivos grandes:")
                col1, col2 = st.columns(2)
                
                with col1:
                    st.info("**üí° Sugest√µes:**")
                    st.markdown("""
                    - Divida o arquivo em partes menores
                    - Use apenas uma amostra dos dados
                    - Remova colunas desnecess√°rias
                    - Comprima o arquivo (ZIP)
                    """)
                
                with col2:
                    st.warning("**‚ö†Ô∏è Limites do sistema:**")
                    st.markdown(f"""
                    - **Tamanho atual:** {file_size_mb:.1f}MB
                    - **Limite m√°ximo:** 200MB
                    - **Excesso:** {file_size_mb - 200:.1f}MB
                    """)
                
                # Bot√£o para tentar novamente
                if st.button("üîÑ Tentar com outro arquivo", key="try_another_file"):
                    st.session_state.clear()
                    st.rerun()
                
                return
            
            # Mostrar progresso para arquivos grandes
            if file_size_mb > 2:
                progress_bar = st.progress(0)
                status_text = st.empty()
                status_text.text("Iniciando processamento...")
            
            try:
                import time
                
                if file_size_mb > 5:
                    st.warning(f"‚è±Ô∏è Arquivo grande ({file_size_mb:.1f}MB). Aplicando processamento otimizado em chunks.")
                elif file_size_mb > 2:
                    st.info(f"üìä Processando arquivo de {file_size_mb:.1f}MB...")
                
                start_time = time.time()
                
                # Atualizar progresso
                if file_size_mb > 2:
                    progress_bar.progress(20)
                    status_text.text("Carregando arquivo...")
                
                df = load_csv_file(uploaded_file)
                
                if file_size_mb > 2:
                    progress_bar.progress(80)
                    status_text.text("Finalizando...")
                
                processing_time = time.time() - start_time
                
                if file_size_mb > 2:
                    progress_bar.progress(100)
                    status_text.text("Conclu√≠do!")
                    time.sleep(0.5)  # Mostrar conclus√£o brevemente
                    progress_bar.empty()
                    status_text.empty()
                
                if df is not None:
                    st.success(f"‚úÖ CSV carregado com sucesso! Shape: {df.shape}")
                    st.info(f"‚è±Ô∏è Tempo de processamento: {processing_time:.2f}s")
                    st.session_state.df = df
                    st.session_state.current_file_info = current_file_info
                    
                    # Carregar dados no agente se j√° inicializado
                    if st.session_state.agent is not None:
                        try:
                            with st.spinner("Carregando dados no agente..."):
                                st.session_state.agent.load_csv(df=df)
                                st.success("‚úÖ Dados carregados no agente com sucesso!")
                        except Exception as e:
                            st.error(f"Erro ao carregar dados no agente: {str(e)}")
                            st.error(f"Detalhes: {traceback.format_exc()}")
                else:
                    st.error("‚ùå Erro ao carregar o arquivo CSV")
                    
            except TimeoutError:
                st.error("‚è±Ô∏è Timeout: Arquivo muito grande para processar. Tente uma amostra menor.")
            except MemoryError:
                st.error("üíæ Erro de mem√≥ria: Arquivo muito grande. Tente uma amostra menor.")
            except Exception as e:
                st.error(f"‚ùå Erro inesperado ao processar arquivo: {str(e)}")
                st.error("üí° Tente recarregar a p√°gina ou usar um arquivo menor.")
    
    # Interface principal
    if st.session_state.df is not None:
        
        # Tabs para organizar conte√∫do
        tab1, tab2, tab3, tab4, tab5 = st.tabs([
            "üìã Vis√£o Geral", 
            "üîç An√°lise Autom√°tica", 
            "üìä Visualiza√ß√µes", 
            "‚ùì Perguntas", 
            "üí° Conclus√µes"
        ])
        
        with tab1:
            display_dataset_overview(st.session_state.df)
        
        with tab2:
            if st.session_state.agent is not None:
                display_automatic_analysis(st.session_state.agent)
            else:
                st.warning("‚ö†Ô∏è Funcionalidade requer agente inicializado com API key v√°lida")
        
        with tab3:
            if st.session_state.agent is not None:
                display_visualizations(st.session_state.agent)
            else:
                st.warning("‚ö†Ô∏è Funcionalidade requer agente inicializado com API key v√°lida")
        
        with tab4:
            st.markdown('<div class="section-header">‚ùì Fa√ßa Perguntas sobre os Dados</div>', unsafe_allow_html=True)
            
            if st.session_state.agent is not None:
                # Exemplos de perguntas
                st.markdown("**Exemplos de perguntas que voc√™ pode fazer:**")
                examples = [
                    "Qual a distribui√ß√£o da vari√°vel Class?",
                    "Existem correla√ß√µes fortes entre as vari√°veis?",
                    "Quais s√£o os outliers na coluna Amount?",
                    "Qual a m√©dia e mediana de cada vari√°vel num√©rica?",
                    "Existem padr√µes temporais nos dados?",
                    "Quais s√£o as principais caracter√≠sticas do dataset?"
                ]
                
                for example in examples:
                    if st.button(f"üí° {example}", key=f"example_{hash(example)}"):
                        handle_user_question(st.session_state.agent, example)
                
                st.markdown("---")
                
                # Input para pergunta personalizada
                user_question = st.text_input(
                    "Sua pergunta:",
                    placeholder="Digite sua pergunta sobre os dados...",
                    key="user_question"
                )
                
                if st.button("üöÄ Analisar", type="primary"):
                    if user_question:
                        handle_user_question(st.session_state.agent, user_question)
                    else:
                        st.warning("Por favor, digite uma pergunta.")
                
                # Hist√≥rico de perguntas
                if st.session_state.analysis_history:
                    st.markdown("---")
                    st.markdown("**üìö Hist√≥rico de An√°lises:**")
                    
                    for i, item in enumerate(reversed(st.session_state.analysis_history[-5:]), 1):
                        with st.expander(f"Pergunta {len(st.session_state.analysis_history) - i + 1}: {item['question'][:50]}..."):
                            st.write(f"**Pergunta:** {item['question']}")
                            st.write(f"**Resposta:** {item['response']}")
                            if item['has_visualization']:
                                st.write("üìä *Visualiza√ß√£o gerada*")
            else:
                st.warning("‚ö†Ô∏è Funcionalidade requer agente inicializado com API key v√°lida")
                st.info("Configure uma API key v√°lida da OpenAI para usar as funcionalidades de IA")
        
        with tab5:
            if st.session_state.agent is not None:
                display_conclusions(st.session_state.agent)
            else:
                st.warning("‚ö†Ô∏è Funcionalidade requer agente inicializado com API key v√°lida")
    
    else:
        # Tela inicial
        st.markdown("""
        ## üöÄ Bem-vindo ao Agente de An√°lise CSV!
        
        Este agente inteligente pode analisar qualquer arquivo CSV e responder perguntas sobre seus dados.
        
        ### üéØ Funcionalidades:
        - **An√°lise Explorat√≥ria Autom√°tica**: Estat√≠sticas, distribui√ß√µes, correla√ß√µes
        - **Detec√ß√£o de Outliers**: Identifica√ß√£o de valores at√≠picos
        - **Visualiza√ß√µes Interativas**: Gr√°ficos autom√°ticos baseados nas perguntas
        - **Processamento de Linguagem Natural**: Fa√ßa perguntas em portugu√™s
        - **Sistema de Mem√≥ria**: O agente lembra das an√°lises anteriores
        - **Gera√ß√£o de Insights**: Conclus√µes autom√°ticas sobre os dados
        
        ### üìù Como usar:
        1. Configure sua OpenAI API Key na barra lateral
        2. Fa√ßa upload de um arquivo CSV
        3. Explore as an√°lises autom√°ticas
        4. Fa√ßa perguntas sobre os dados
        5. Veja as conclus√µes geradas pelo agente
        
        ### üîß Configura√ß√£o:
        - Crie um arquivo `.env` com sua `OPENAI_API_KEY`
        - Ou insira a chave diretamente na barra lateral
        """)

def partition_large_file(uploaded_file, max_partition_size_mb=10):
    """
    Particiona automaticamente arquivos grandes em partes menores para processamento otimizado.
    Otimizado para arquivos de at√© 200MB no Render
    
    Args:
        uploaded_file: Arquivo carregado pelo Streamlit
        max_partition_size_mb: Tamanho m√°ximo de cada parti√ß√£o em MB
    
    Returns:
        list: Lista de DataFrames representando as parti√ß√µes
    """
    file_size_mb = uploaded_file.size / (1024 * 1024)
    
    # Calcular tamanho ideal da parti√ß√£o baseado no tamanho do arquivo
    if file_size_mb > 100:
        partition_size = 2000  # Parti√ß√µes muito pequenas para arquivos gigantes
        max_rows = 20000  # Limite m√°ximo de linhas para arquivos muito grandes
    elif file_size_mb > 50:
        partition_size = 3000  # Parti√ß√µes pequenas para arquivos grandes
        max_rows = 30000  # Limite m√°ximo de linhas
    elif file_size_mb > 30:
        partition_size = 5000  # Parti√ß√µes m√©dias
        max_rows = 50000  # Limite m√°ximo de linhas
    else:
        partition_size = 8000  # Parti√ß√µes maiores para arquivos menores
        max_rows = 100000  # Limite m√°ximo de linhas
    
    if file_size_mb <= max_partition_size_mb:
        # Arquivo pequeno, n√£o precisa particionar
        df = pd.read_csv(uploaded_file, low_memory=False)
        # Aplicar limite de linhas mesmo para arquivos pequenos
        if len(df) > max_rows:
            df = df.head(max_rows)
            st.warning(f"‚ö†Ô∏è Dataset limitado a {max_rows:,} linhas para otimizar performance.")
        return [df]
    
    st.info(f"üìä Configura√ß√£o: Parti√ß√µes de {partition_size:,} linhas (m√°x: {max_rows:,} linhas)")
    
    # Primeiro, descobrir o n√∫mero total de linhas de forma mais eficiente
    uploaded_file.seek(0)
    # Estimar n√∫mero de linhas baseado em amostra
    sample_lines = 0
    sample_bytes = 0
    for line in uploaded_file:
        sample_lines += 1
        sample_bytes += len(line)
        if sample_lines >= 1000:  # Amostra de 1000 linhas
            break
    
    # Estimar total de linhas
    avg_line_size = sample_bytes / sample_lines
    estimated_total_lines = int(uploaded_file.size / avg_line_size) - 1  # -1 para excluir cabe√ßalho
    
    # Aplicar limite m√°ximo de linhas
    total_lines_to_process = min(estimated_total_lines, max_rows)
    
    uploaded_file.seek(0)
    
    # Calcular n√∫mero de parti√ß√µes baseado no tamanho da parti√ß√£o
    num_partitions = math.ceil(total_lines_to_process / partition_size)
    
    st.info(f"üîÑ Arquivo grande detectado ({file_size_mb:.1f}MB). Dividindo em {num_partitions} parti√ß√µes...")
    st.info(f"üìä Estimativa: {estimated_total_lines:,} linhas | Processando: {total_lines_to_process:,} linhas")
    
    partitions = []
    
    # Container para progress bar
    progress_container = st.container()
    with progress_container:
        partition_progress = st.progress(0)
        partition_status = st.empty()
    
    # Ler arquivo em parti√ß√µes
    total_rows_read = 0
    for i in range(num_partitions):
        start_row = i * partition_size
        
        # Calcular quantas linhas ler nesta parti√ß√£o
        remaining_rows = total_lines_to_process - total_rows_read
        nrows = min(partition_size, remaining_rows)
        
        if nrows <= 0:
            break
        
        try:
            # Resetar ponteiro do arquivo
            uploaded_file.seek(0)
            
            # Ler parti√ß√£o
            if start_row == 0:
                # Primeira parti√ß√£o - incluir cabe√ßalho
                partition_df = pd.read_csv(uploaded_file, nrows=nrows, low_memory=False)
            else:
                # Parti√ß√µes subsequentes - pular linhas anteriores
                partition_df = pd.read_csv(uploaded_file, skiprows=range(1, start_row + 1), nrows=nrows, low_memory=False)
            
            partitions.append(partition_df)
            total_rows_read += len(partition_df)
            
            # Atualizar progress bar
            progress_percent = (i + 1) / num_partitions
            partition_progress.progress(progress_percent)
            partition_status.text(f"üì¶ Parti√ß√£o {i+1}/{num_partitions} processada: {len(partition_df):,} linhas")
            
        except Exception as e:
            st.error(f"‚ùå Erro ao processar parti√ß√£o {i+1}: {str(e)}")
            break
    
    partition_status.text(f"‚úÖ {len(partitions)} parti√ß√µes criadas com sucesso! Total: {total_rows_read:,} linhas")
    
    return partitions

def process_partitioned_data(partitions, operation="analyze", max_rows=50000):
    """
    Processa dados particionados de forma sequencial.
    Otimizado para arquivos grandes no Render
    
    Args:
        partitions: Lista de DataFrames (parti√ß√µes)
        operation: Tipo de opera√ß√£o ("analyze", "visualize", "summary")
        max_rows: Limite m√°ximo de linhas para processamento
    
    Returns:
        dict: Resultados consolidados do processamento
    """
    if not partitions:
        return None
    
    st.info(f"üîÑ Processando {len(partitions)} parti√ß√µes sequencialmente...")
    
    results = {
        'total_rows': 0,
        'total_columns': partitions[0].shape[1] if partitions else 0,
        'column_names': partitions[0].columns.tolist() if partitions else [],
        'data_types': {},
        'summary_stats': {},
        'combined_sample': None
    }
    
    # Container para progress bar
    progress_container = st.container()
    with progress_container:
        process_progress = st.progress(0)
        process_status = st.empty()
    
    # Processar cada parti√ß√£o
    for i, partition in enumerate(partitions):
        try:
            # Atualizar contadores
            results['total_rows'] += len(partition)
            
            # Primeira parti√ß√£o - estabelecer estrutura base
            if i == 0:
                results['data_types'] = partition.dtypes.to_dict()
                results['summary_stats'] = partition.describe().to_dict()
                results['combined_sample'] = partition.head(1000).copy()
            else:
                # Parti√ß√µes subsequentes - combinar amostras
                if results['combined_sample'] is not None and len(results['combined_sample']) < 5000:
                    sample_size = min(1000, len(partition))
                    additional_sample = partition.head(sample_size)
                    results['combined_sample'] = pd.concat([results['combined_sample'], additional_sample], ignore_index=True)
            
            # Atualizar progress bar
            progress_percent = (i + 1) / len(partitions)
            process_progress.progress(progress_percent)
            process_status.text(f"‚öôÔ∏è Processando parti√ß√£o {i+1}/{len(partitions)}: {len(partition):,} linhas")
            
        except Exception as e:
            st.error(f"‚ùå Erro ao processar parti√ß√£o {i+1}: {str(e)}")
            continue
    
    process_status.text(f"‚úÖ Processamento conclu√≠do: {results['total_rows']:,} linhas totais")
    
    return results

if __name__ == "__main__":
    main()